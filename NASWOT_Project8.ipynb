{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NASWOT Project8.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u0Nq2-kqUpgm",
        "Y4fCjA8nZW7T",
        "CW5ewvBLU1OE",
        "aQPU44nMdd3F",
        "ayOrdzLJtH6p"
      ],
      "authorship_tag": "ABX9TyM2klKOJrP6ITn9I22t3UeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f0b2d423c4c43f6ae6df0ba63aa2c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f6bb762489e4992aaf86bb0ccd53b6a",
              "IPY_MODEL_67bf6e03406a410a97788042224af9a7",
              "IPY_MODEL_e1cf46b89e4e4566a3c9b44bafa90d73"
            ],
            "layout": "IPY_MODEL_28382584336f415696cc7d54100c6d31"
          }
        },
        "3f6bb762489e4992aaf86bb0ccd53b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7761af1712854e29bb84c966a57222d3",
            "placeholder": "​",
            "style": "IPY_MODEL_ca4319607df8416ba9330d71f3d3cd1d",
            "value": "100%"
          }
        },
        "67bf6e03406a410a97788042224af9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e552e16ec71d40b3b293d00a557b7271",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_748ca2c22274402dbe0b924d94558530",
            "value": 169001437
          }
        },
        "e1cf46b89e4e4566a3c9b44bafa90d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff5aeae657b49e58f0cea5e11a0e7d5",
            "placeholder": "​",
            "style": "IPY_MODEL_1a9825a163084181b56ee89f98f167c5",
            "value": " 169001437/169001437 [00:01&lt;00:00, 95989713.29it/s]"
          }
        },
        "28382584336f415696cc7d54100c6d31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7761af1712854e29bb84c966a57222d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4319607df8416ba9330d71f3d3cd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e552e16ec71d40b3b293d00a557b7271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748ca2c22274402dbe0b924d94558530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ff5aeae657b49e58f0cea5e11a0e7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a9825a163084181b56ee89f98f167c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelePresti/NAS_MachineLearningDeepLearning/blob/main/NASWOT_Project8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Nq2-kqUpgm"
      },
      "source": [
        "#Define Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofIogUx_T3lD"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "max_uid = 15625\n",
        "#@title ##Configuration Info { run: \"auto\" }\n",
        "#configuration by param\n",
        "api_loading_mode = \"Lite\" #@param {type: \"string\"} [\"Lite\", \"Full\"]\n",
        "dataset = \"cifar100\" #@param {type:\"string\"} [\"cifar10\", \"cifar100\", \"ImageNet16\"]\n",
        "run_id =  1# @param {type:\"integer\"}\n",
        "trial =  2#@param {type:\"integer\"}\n",
        "n_random =  10#@param {type:\"integer\"}\n",
        "point = '2a' # @param ['2a', '2b']\n",
        "imagenet_path = 'Use only if dataset=Imagenet16' #@param{type:\"string\"}\n",
        "use_default_path = True #@param{type:\"boolean\"}\n",
        "n_evolution = 2#@param{type: \"integer\"}\n",
        "n_arch_distance = 2#@param{type: \"integer\"}\n",
        "n_survivor = 1#@param{type:\"integer\"}\n",
        "population_size = 10#@param{type:\"integer\"}\n",
        "proxy_type = \"ReLU\" #@param {type: \"string\"} [\"ReLU\", \"SynFlow\"]\n",
        "CIFAR10 = 'cifar10'\n",
        "CIFAR100 = 'cifar100'\n",
        "IMAGENET = 'ImageNet16'\n",
        "\n",
        "\n",
        "config['score'] = 'hook_logdet'\n",
        "config['nasspace'] = 'nasbench201'\n",
        "config['augtype'] = 'none'\n",
        "config['dataset'] = dataset\n",
        "config['maxofn'] = 3\n",
        "config['batch_size'] = 128\n",
        "config['seed'] = 1\n",
        "config['run_id'] = run_id\n",
        "config['dataset_id'] = 'CIFAR10'\n",
        "config['start_uid'] = 0 \n",
        "config['stop_uid'] =  15000 \n",
        "config['trial'] = trial\n",
        "config['n_random'] = n_random\n",
        "config['point'] = point\n",
        "config['imagenet_path'] = '/content/drive/MyDrive/ImageNet16' if use_default_path else imagenet_path\n",
        "config['n_evolution'] = n_evolution\n",
        "config['n_arch_distance'] = n_arch_distance\n",
        "config['n_survivor'] = n_survivor\n",
        "config['population_size'] = population_size\n",
        "config['proxy_type'] = proxy_type\n",
        "config['api_loading_mode'] = api_loading_mode\n",
        "\n",
        "#max 15625 stop_uid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h1xetWeZeEDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f2ea00-4e7c-4740-8d61-82f2fb9e63ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/MichelePresti/NAS_MachineLearningDeepLearning"
      ],
      "metadata": {
        "id": "LaYbzg0SY_rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/NAS_MachineLearningDeepLearning/neural_model .\n",
        "!cp -r /content/NAS_MachineLearningDeepLearning/ZeroCostNas ."
      ],
      "metadata": {
        "id": "UrTGVZKZZCJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing NasBenchAPI ✍"
      ],
      "metadata": {
        "id": "Y4fCjA8nZW7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_arch_config_by_dataset(dataset) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function return the architectures config by dataset in a pandas dataframe.\n",
        "    PARAMETERS:\n",
        "       dataset= string among [cifar10, cifar100, imaginet]\n",
        "    \"\"\"\n",
        "    if(dataset == 'cifar10'):\n",
        "        df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__CIFAR10_config.csv', header=0)\n",
        "        return df\n",
        "    if(dataset == 'cifar100'):\n",
        "      df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__CIFAR100_config.csv', header=0)\n",
        "      return df\n",
        "    if(dataset == 'ImageNet16'):\n",
        "      df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__ImageNet16_config.csv', header=0)\n",
        "      return df\n",
        "    else: \n",
        "      print('Dataset name not valid')\n",
        "      return None\n",
        "\n",
        "def get_standard_config(csv_config: pd.DataFrame) -> dict:\n",
        "    res = {}\n",
        "    res['name'] = csv_config.iloc[0]['name']\n",
        "    res['C'] = csv_config.iloc[0]['C']\n",
        "    res['N'] = csv_config.iloc[0]['N']\n",
        "    res['arch_str'] = csv_config.iloc[0]['arch_str']\n",
        "    res['num_classes'] = 1\n",
        "    return res"
      ],
      "metadata": {
        "id": "q2Mp9xSfaCnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "if config['api_loading_mode'] == 'Lite':\n",
        "  searchspace = get_arch_config_by_dataset(config['dataset'])\n",
        "else:\n",
        "  # To be implemented the full version (Loading the NASBench201 API)\n",
        "  pass\n",
        "\n",
        "print('SearchSpace Loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CjQoImMajYM",
        "outputId": "07ca7777-582d-4389-8914-103e1a8212db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SearchSpace Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW5ewvBLU1OE"
      },
      "source": [
        "#Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s65hVshwUCVo"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019 #\n",
        "##################################################\n",
        "import os, sys, hashlib, torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "\n",
        "def calculate_md5(fpath, chunk_size=1024 * 1024):\n",
        "    md5 = hashlib.md5()\n",
        "    with open(fpath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "\n",
        "def check_md5(fpath, md5, **kwargs):\n",
        "    return md5 == calculate_md5(fpath, **kwargs)\n",
        "\n",
        "\n",
        "def check_integrity(fpath, md5=None):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    if md5 is None:\n",
        "        return True\n",
        "    else:\n",
        "        return check_md5(fpath, md5)\n",
        "\n",
        "\n",
        "class ImageNet16(data.Dataset):\n",
        "    # http://image-net.org/download-images\n",
        "    # A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets\n",
        "    # https://arxiv.org/pdf/1707.08819.pdf\n",
        "\n",
        "    train_list = [\n",
        "        [\"train_data_batch_1\", \"27846dcaa50de8e21a7d1a35f30f0e91\"],\n",
        "        [\"train_data_batch_2\", \"c7254a054e0e795c69120a5727050e3f\"],\n",
        "        [\"train_data_batch_3\", \"4333d3df2e5ffb114b05d2ffc19b1e87\"],\n",
        "        [\"train_data_batch_4\", \"1620cdf193304f4a92677b695d70d10f\"],\n",
        "        [\"train_data_batch_5\", \"348b3c2fdbb3940c4e9e834affd3b18d\"],\n",
        "        [\"train_data_batch_6\", \"6e765307c242a1b3d7d5ef9139b48945\"],\n",
        "        [\"train_data_batch_7\", \"564926d8cbf8fc4818ba23d2faac7564\"],\n",
        "        [\"train_data_batch_8\", \"f4755871f718ccb653440b9dd0ebac66\"],\n",
        "        [\"train_data_batch_9\", \"bb6dd660c38c58552125b1a92f86b5d4\"],\n",
        "        [\"train_data_batch_10\", \"8f03f34ac4b42271a294f91bf480f29b\"],\n",
        "    ]\n",
        "    valid_list = [\n",
        "        [\"val_data\", \"3410e3017fdaefba8d5073aaa65e4bd6\"],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, train, transform, use_num_of_class_only=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.train = train  # training set or valid set\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted.\")\n",
        "\n",
        "        if self.train:\n",
        "            downloaded_list = self.train_list\n",
        "        else:\n",
        "            downloaded_list = self.valid_list\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        for i, (file_name, checksum) in enumerate(downloaded_list):\n",
        "            file_path = os.path.join(self.root, file_name)\n",
        "            # print ('Load {:}/{:02d}-th : {:}'.format(i, len(downloaded_list), file_path))\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(f)\n",
        "                else:\n",
        "                    entry = pickle.load(f, encoding=\"latin1\")\n",
        "                self.data.append(entry[\"data\"])\n",
        "                self.targets.extend(entry[\"labels\"])\n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 16, 16)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        if use_num_of_class_only is not None:\n",
        "            assert (\n",
        "                isinstance(use_num_of_class_only, int)\n",
        "                and use_num_of_class_only > 0\n",
        "                and use_num_of_class_only < 1000\n",
        "            ), \"invalid use_num_of_class_only : {:}\".format(use_num_of_class_only)\n",
        "            new_data, new_targets = [], []\n",
        "            for I, L in zip(self.data, self.targets):\n",
        "                if 1 <= L <= use_num_of_class_only:\n",
        "                    new_data.append(I)\n",
        "                    new_targets.append(L)\n",
        "            self.data = new_data\n",
        "            self.targets = new_targets\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{name}({num} images, {classes} classes)\".format(\n",
        "            name=self.__class__.__name__,\n",
        "            num=len(self.data),\n",
        "            classes=len(set(self.targets)),\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index] - 1\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in self.train_list + self.valid_list:\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpvhhftTUCTS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "def get_dataset(dataset) -> DataLoader:\n",
        "    \"\"\"\n",
        "    This function return the dataset given its name in torch DataLoader format.\n",
        "    PARAMETERS:\n",
        "       dataset= string among [cifar10, cifar100, imaginet]\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset == 'cifar10':\n",
        "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    elif dataset == 'cifar100':\n",
        "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
        "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    elif dataset.startswith('ImageNet16'):\n",
        "        mean = [x / 255 for x in [122.68, 116.66, 104.01]]\n",
        "        std = [x / 255 for x in [63.22, 61.26, 65.09]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(16, padding=2), transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = ImageNet16(config['imagenet_path'], True, transform, 120)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    else:\n",
        "        raise TypeError(\"Unknow dataset : {:}\".format(dataset))\n",
        "\n",
        "    return train_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZRyJjrpUCQn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "9f0b2d423c4c43f6ae6df0ba63aa2c57",
            "3f6bb762489e4992aaf86bb0ccd53b6a",
            "67bf6e03406a410a97788042224af9a7",
            "e1cf46b89e4e4566a3c9b44bafa90d73",
            "28382584336f415696cc7d54100c6d31",
            "7761af1712854e29bb84c966a57222d3",
            "ca4319607df8416ba9330d71f3d3cd1d",
            "e552e16ec71d40b3b293d00a557b7271",
            "748ca2c22274402dbe0b924d94558530",
            "2ff5aeae657b49e58f0cea5e11a0e7d5",
            "1a9825a163084181b56ee89f98f167c5"
          ]
        },
        "outputId": "a6724f31-195a-4965-ec71-76dcd54f16e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f0b2d423c4c43f6ae6df0ba63aa2c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "train_dt = get_dataset(config['dataset'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "if config['dataset'] == 'cifar10':\n",
        "  df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/Cifar10Result.csv')\n",
        "elif config['dataset'] == 'cifar100':\n",
        "  df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/Cifar100Result.csv')\n",
        "else:\n",
        "  df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/ImageNet16Result.csv')\n",
        "\n",
        "try: \n",
        "  df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "except:\n",
        "  print(\"Already dropped\")\n",
        "\n",
        "acc_df = df\n"
      ],
      "metadata": {
        "id": "voCzkEUpcyDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NASWOT Scoring Algorithm ➗"
      ],
      "metadata": {
        "id": "pz6sd7QLgbv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_batch_jacobian(net, x, target, device, args=None):\n",
        "    net.zero_grad()\n",
        "    x.requires_grad_(True)\n",
        "    y, out = net(x)\n",
        "    y.backward(torch.ones_like(y))\n",
        "    jacob = x.grad.detach()\n",
        "    return jacob, target.detach(), y.detach(), out.detach()\n",
        "\n",
        "\n",
        "def hooklogdet(K, labels=None):\n",
        "    s, ld = np.linalg.slogdet(K)\n",
        "    return ld\n",
        "\n",
        "\n",
        "def score_network(network, x, x2, target, device):\n",
        "    jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "    network(x2.to(device))\n",
        "    value = hooklogdet(network.K, target)\n",
        "    return value"
      ],
      "metadata": {
        "id": "hqNoimRxgs2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from neural_model.neural_model import get_cell_net\n",
        "\n",
        "\"\"\"\n",
        "NAS WOT Algorithm\n",
        "\"\"\"\n",
        "\n",
        "def naswot_search(dataset, device, population, run_id=-1) -> pd.DataFrame:\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': [], 'accuracy': [], 'run_id':[]}\n",
        "    for uid in population:\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "          network = network.to(device)\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "          print(f'Score (uid {uid}): {np.mean(s)}, Accuracy: {acc}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['accuracy'].append(acc)\n",
        "          result['run_id'].append(run_id)\n",
        "          score = np.mean(s)\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': [], 'run_id':[]}\n",
        "    return df"
      ],
      "metadata": {
        "id": "Re13w9yFgr8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Point 2 Project 8 (Random Search) ➰\n",
        "\n",
        "---\n",
        "\n",
        "A. Run 30 random search experiments on NASWOT algorithm, storing results about score, accuracy and time.\n",
        "\n",
        "B. For each of the above experiments store the best performing architecture."
      ],
      "metadata": {
        "id": "aQPU44nMdd3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ResultToSave"
      ],
      "metadata": {
        "id": "vDgpm_NRw3i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Point 2 Configuration { run: \"auto\" }\n",
        "#configuration by param\n",
        "n_trial =  30#@param {type:\"integer\"}\n",
        "n_population =  1000#@param {type:\"integer\"}\n",
        "run_save_path = 'NASWOT_Point2a'#@param {type:\"string\"}\n",
        "best_save_path = 'NASWOT_Point2b'#@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "DTt7RgSoplBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ['WANDB_CONSOLE'] = 'off'\n",
        "start = time.time()\n",
        "\n",
        "run_id = config['run_id']\n",
        "dataset = config['dataset']\n",
        "n = n_population   # N size of random sample\n",
        "trial = n_trial\n",
        "best = {}\n",
        "\n",
        "print('*******************************')\n",
        "print('Running Random Search algorithm')\n",
        "print('Parameters:')\n",
        "print(f'Dataset: {dataset}')\n",
        "print(f'Num Round: {trial}')\n",
        "print(f'Population Size: {n}')\n",
        "print('*******************************')\n",
        "\n",
        "\n",
        "for i in range(trial):\n",
        "  print(f\"Round {i}\")\n",
        "\n",
        "  # Sample N Random architectures among the searchspace\n",
        "  population = random.sample(range(max_uid), n)\n",
        "\n",
        "  # Train Population\n",
        "  trained_population = naswot_search(dataset=dataset, device=device, population=population, run_id=i)\n",
        "\n",
        "  # Save Training Results\n",
        "  trained_population.to_csv(f'ResultToSave/{run_save_path}_RunID_{i}_Dataset_{dataset}.csv')\n",
        "  trained_population.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "  if len(best) > 0:\n",
        "    best: pd.DataFrame = best.append(trained_population.head(1), ignore_index=True)\n",
        "  else:\n",
        "    best = trained_population.head(config['n_survivor'])\n",
        "\n",
        "best.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "best_of_all = best.head(1)\n",
        "\n",
        "best.to_csv(f'ResultToSave/{best_save_path}_Dataset_{dataset}.csv')\n",
        "stop = time.time()\n",
        "\n",
        "total_time = stop - start\n",
        "print('*****************************************************************')\n",
        "print(f'Best performing net with RandomSearch')\n",
        "print(tabulate(best_of_all, headers='keys', tablefmt='psql', showindex=False))\n",
        "print(f'Total time for search over all searchspace: {total_time}')\n",
        "print('*****************************************************************')\n"
      ],
      "metadata": {
        "id": "t-i6yAbHgWPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Results\n",
        "import pandas as pd\n",
        "import os, fnmatch\n",
        "\n",
        "dataset = config['dataset']\n",
        "def find(pattern, path):\n",
        "    result = []\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for name in files:\n",
        "            if fnmatch.fnmatch(name, pattern):\n",
        "                result.append(os.path.join(root, name))\n",
        "    return result\n",
        "\n",
        "files_2a = find(f'{run_save_path}_*', '/content/ResultToSave/')\n",
        "files_2b = find(f'{best_save_path}_*', '/content/ResultToSave/')\n",
        "\n",
        "df = []\n",
        "for file in files_2a:\n",
        "  tmp = pd.read_csv(file, header=0)\n",
        "  file_name = file.split('/')[-1]\n",
        "  tmp.to_csv(f'/content/drive/MyDrive/RisultatiNASWOT/{dataset}/{file_name}')\n",
        "\n",
        "for file in files_2b:\n",
        "  tmp = pd.read_csv(file, header=0)\n",
        "  file_name = file.split('/')[-1]\n",
        "  tmp.to_csv(f'/content/drive/MyDrive/RisultatiNASWOT/{dataset}/{file_name}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tghjNHiTvKem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synflow Score/Validation Accuracy Correlation ⏳\n",
        "\n",
        "___\n",
        "\n",
        "In this section the correlation between Synflow Score and Validation Accuracy on each datasets is evaluated."
      ],
      "metadata": {
        "id": "ayOrdzLJtH6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and store synflow score over all architectures of the searchspace\n",
        "import pandas as pd\n",
        "from ZeroCostNas.foresight.models import *\n",
        "from ZeroCostNas.foresight.pruners import *\n",
        "from ZeroCostNas.foresight.dataset import *\n",
        "from ZeroCostNas.foresight.weight_initializers import init_net\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_num_classes(dataset):\n",
        "    return 100 if dataset == 'cifar100' else 10 if dataset == 'cifar10' else 120\n",
        "\n",
        "for dataset in ['cifar10', 'cifar100', 'ImageNet16']:\n",
        "  dataloader = get_dataset(dataset)\n",
        "  if dataset == 'cifar10':\n",
        "    accuracy_list = pd.read_csv('/content/NAS_MachineLearningDeepLearning/Cifar10Result.csv')\n",
        "  elif dataset == 'cifar100':\n",
        "    accuracy_list = pd.read_csv('/content/NAS_MachineLearningDeepLearning/Cifar100Result.csv')\n",
        "  else:\n",
        "    accuracy_list = pd.read_csv('/content/NAS_MachineLearningDeepLearning/ImageNet16Result.csv')\n",
        "  dataset_result = {'UID': [], 'Score': [], 'Accuracy': []}\n",
        "  for uid in range(max_uid):\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network =  nasbench2.get_model_from_arch_str(net_config['arch_str'], get_num_classes(dataset))\n",
        "      network.to(device)\n",
        "      init_net(network, 'none', 'none')\n",
        "      measures = predictive.find_measures(network, dataloader, ('random', 1, get_num_classes(dataset)), device)\n",
        "      print(measures)\n",
        "      dataset_result['UID'].append(uid)\n",
        "      dataset_result['Score'].append(measures['synflow'])\n",
        "      dataset_result['Accuracy'].append(accuracy_list['valid-accuracy'].iloc[uid])\n",
        "  dataset_result = pd.DataFrame(dataset_result)\n",
        "  dataset_result.to_csv(f'SynflowScore_{dataset}.csv')\n"
      ],
      "metadata": {
        "id": "BeOqqxoauCBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uWZajyfvy7-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aging Evolution Algorithm (Synflow Proxy) 👾\n",
        "\n",
        "---\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "1. Get N Random Architectures Called Population\n",
        "2. Run Scoring Algorithm Based On Synflow Proxy Score\n",
        "3. Choose the best architecture and mutate in order to generate a new one\n",
        "4. Pop off the oldest architecture\n",
        "5. Repeat for N Evolution Stage"
      ],
      "metadata": {
        "id": "OZQV3CVfEwqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Synflow Proxy Configuration { run: \"auto\" }\n",
        "#configuration by param\n",
        "n_trial =  50#@param {type:\"integer\"}\n",
        "save_path = 'NASWOT_AgingEvolution_Synflow'#@param {type:\"string\"}\n",
        "max_trained_models=1000 #@param {type: 'integer'}\n",
        "pool_size=64 #@param {type: 'integer'}\n",
        "tournament_size=20 #@param {type: 'integer'} \n",
        "preload_score = True #@param{type:\"boolean\"}"
      ],
      "metadata": {
        "id": "IkQnoYqLGA8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Synflow NasBench201 Score\n",
        "\n",
        "import pickle\n",
        "\n",
        "synflow_proxy=[]\n",
        "\n",
        "file_path = ''\n",
        "dataset = config['dataset']\n",
        "\n",
        "if dataset == CIFAR10:\n",
        "  file_path = '/content/drive/MyDrive/results_release/nasbench2/nb2_cf10_seed42_dlrandom_dlinfo1_initwnone_initbnone.p'\n",
        "elif dataset == CIFAR100:\n",
        "  file_path = '/content/drive/MyDrive/results_release/nasbench2/nb2_cf100_seed42_dlrandom_dlinfo1_initwnone_initbnone.p'\n",
        "else:\n",
        "  file_path = '/content/drive/MyDrive/results_release/nasbench2/nb2_im120_seed42_dlrandom_dlinfo1_initwnone_initbnone.p'\n",
        "\n",
        "f = open(file_path,'rb')\n",
        "while(1):\n",
        "    try:\n",
        "        d = pickle.load(f)\n",
        "        synflow_proxy.append(d['logmeasures']['synflow'])\n",
        "    except EOFError:\n",
        "        break\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "CmGzNFLWLmXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Coded Architectures structures\n",
        "\n",
        "_opname_to_index = {\n",
        "    'none': 0,\n",
        "    'skip_connect': 1,\n",
        "    'nor_conv_1x1': 2,\n",
        "    'nor_conv_3x3': 3,\n",
        "    'avg_pool_3x3': 4\n",
        "}\n",
        "\n",
        "def get_spec_from_arch_str(arch_str):\n",
        "    nodes = arch_str.split('+')\n",
        "    nodes = [node[1:-1].split('|') for node in nodes]\n",
        "    nodes = [[op_and_input.split('~')[0]  for op_and_input in node] for node in nodes]\n",
        "\n",
        "    spec = [_opname_to_index[op] for node in nodes for op in node]\n",
        "    return spec\n",
        "\n",
        "idx_to_spec = {}\n",
        "for i in range(0, max_uid):\n",
        "    idx_to_spec[i] = get_spec_from_arch_str(searchspace.iloc[i]['arch_str'])\n",
        "\n",
        "spec_to_idx = {}\n",
        "for idx,spec in idx_to_spec.items():\n",
        "    spec_to_idx[str(spec)] = idx"
      ],
      "metadata": {
        "id": "1iCHTLVnNZfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import copy\n",
        "from ZeroCostNas.foresight.models import *\n",
        "from ZeroCostNas.foresight.pruners import *\n",
        "from ZeroCostNas.foresight.dataset import *\n",
        "from ZeroCostNas.foresight.weight_initializers import init_net\n",
        "\n",
        "dataset = config['dataset']\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_num_classes(dataset):\n",
        "    return 100 if dataset == 'cifar100' else 10 if dataset == 'cifar10' else 120\n",
        "\n",
        "def get_synflow_score(uid):\n",
        "  if preload_score:\n",
        "    return synflow_proxy[uid]\n",
        "  else:\n",
        "    net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "    net_config: dict = get_standard_config(net_config)\n",
        "    network =  nasbench2.get_model_from_arch_str(net_config['arch_str'], get_num_classes(dataset))\n",
        "    network.to(device)\n",
        "    init_net(network, 'none', 'none')\n",
        "    measures = predictive.find_measures(network, train_dt, ('random', 1, get_num_classes(dataset)), device)\n",
        "    return measures['synflow']\n",
        "\n",
        "def random_spec():\n",
        "    return random.choice(list(idx_to_spec.values()))\n",
        "\n",
        "def mutate_spec(old_spec):\n",
        "\n",
        "    possible_specs = []\n",
        "    for idx_to_change in range(len(old_spec)): \n",
        "        entry_to_change = old_spec[idx_to_change]\n",
        "        possible_entries = [x for x in range(5) if x != entry_to_change]\n",
        "        for new_entry in possible_entries:\n",
        "            new_spec = copy.copy(old_spec)\n",
        "            new_spec[idx_to_change] = new_entry\n",
        "            possible_specs.append((get_synflow_score(spec_to_idx[str(new_spec)]), new_spec))\n",
        "    best_new_spec = random.choice(possible_specs)[1] # sorted(possible_specs, key=lambda i:i[0])[-1][1]\n",
        "    return best_new_spec\n",
        "\n",
        "def random_combination(pool, sample_size):\n",
        "    indices = sorted(random.sample(range(len(pool)), sample_size))\n",
        "    return pool.iloc[indices]"
      ],
      "metadata": {
        "id": "pxOnX87BOWFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_aging_evolution(max_trained_models, tournament_size, pool):\n",
        "  epochs = 0\n",
        "  best = {'arch': '', 'score': 0, 'acc': }\n",
        "  while epochs < max_trained_models:\n",
        "    sample = random_combination(pool, tournament_size)\n",
        "    # print('SAMPLE', tabulate(sample, headers='keys', tablefmt='psql', showindex=False))\n",
        "    sample.sort_values(by='score', ascending=False, inplace=True)\n",
        "    old_best = sample.head(1)\n",
        "    new_spec = mutate_spec(old_best.iloc[0]['arch'])\n",
        "    uid = spec_to_idx[str(new_spec)]\n",
        "    score = get_synflow_score(uid)\n",
        "    acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "    # Add New Spec \n",
        "    pool = pool.append({\n",
        "        'uid': uid,\n",
        "        'arch': new_spec,\n",
        "        'score': score,\n",
        "        'acc': acc\n",
        "    }, ignore_index=True)\n",
        "    \n",
        "    # Kill Oldest Spec\n",
        "    pool.drop(index=pool.index[0], \n",
        "        axis=0, \n",
        "        inplace=True)\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "  return pool, best"
      ],
      "metadata": {
        "id": "sioHaw5cOhrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def init_population(pool_size):\n",
        "  pool = {'uid': [], 'arch': [], 'score': [], 'acc': []}\n",
        "  for i in range(pool_size):\n",
        "    arch = random_spec()\n",
        "    uid = spec_to_idx[str(arch)]\n",
        "    score = get_synflow_score(uid)\n",
        "    acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "    pool['uid'].append(uid)\n",
        "    pool['arch'].append(arch)\n",
        "    pool['score'].append(score)\n",
        "    pool['acc'].append(acc)\n",
        "  return pd.DataFrame(pool)"
      ],
      "metadata": {
        "id": "Hse56cR3Tjjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pool = init_population(pool_size)\n",
        "pool"
      ],
      "metadata": {
        "id": "iKJBIxT72JNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aging Evolution Algorithm\n",
        "import random\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = config['dataset']\n",
        "trial = n_trial\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "print('*******************************')\n",
        "print('Running Aging Evolution algorithm')\n",
        "print('Parameters:')\n",
        "print(f'Dataset: {dataset}')\n",
        "print(f'Num Round: {trial}')\n",
        "print(f'Max Trained Models: {max_trained_models}')\n",
        "print(f'Pool Size: {pool_size}')\n",
        "print(f'Tournament Size: {tournament_size}')\n",
        "print('*******************************')\n",
        "\n",
        "results = {'UID': [],\n",
        "           'SynflowScore': [],\n",
        "           'ValidationAccuracy': [],\n",
        "           'ArchitectureEncoded': [],\n",
        "           'RunID': []}\n",
        "\n",
        "# Init Population\n",
        "for i in range(trial):\n",
        "  pool = init_population(pool_size)\n",
        "\n",
        "  models = run_aging_evolution(max_trained_models=max_trained_models, tournament_size=tournament_size, pool=pool)\n",
        "\n",
        "  models.sort_values(by='score', ascending=False, inplace=True)\n",
        "  \n",
        "  best = models.head(1)\n",
        "\n",
        "  results['UID'].append(best.iloc[0]['uid'])\n",
        "  results['SynflowScore'].append(best.iloc[0]['score'])\n",
        "  results['ValidationAccuracy'].append(best.iloc[0]['acc'])\n",
        "  results['ArchitectureEncoded'].append(best.iloc[0]['arch'])\n",
        "  results['RunID'].append(i)\n",
        "\n",
        "  print(f'Best Architecture Run {i}')\n",
        "  print(tabulate(best, headers='keys', tablefmt='psql', showindex=False))\n",
        "\n",
        "\n",
        "stop = time.time()\n",
        "elapsed = stop-start\n",
        "\n",
        "print(f'Total time {elapsed}')\n",
        "results = pd.DataFrame(results)\n",
        "results.to_csv(f'{save_path}_{dataset}.csv')\n"
      ],
      "metadata": {
        "id": "HWyTxfUjUaH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized Evolution Algorithm ✨\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "\n",
        "1.   Get N Random Architectures Called Population\n",
        "2.   Run Scoring Algorithm On Population\n",
        "3.   Take N Survivor, Choose As The Best Score\n",
        "4.   Create New Generation With Architecture At N Distance From The Survivor\n",
        "5.   Repeat From 2 For N Evolution Era"
      ],
      "metadata": {
        "id": "ETge1xVOpwhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from neural_model.neural_model import get_cell_net\n",
        "\n",
        "\"\"\"\n",
        "NAS WOT Algorithm\n",
        "\"\"\"\n",
        "\n",
        "def synflow_search(dataset, device, population, run_id=-1) -> pd.DataFrame:\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': [], 'accuracy': [], 'run_id':[]}\n",
        "    for uid in population:\n",
        "      # net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      # net_config: dict = get_standard_config(net_config)\n",
        "      # network = get_cell_net(net_config)\n",
        "      # try:\n",
        "      #     start = time.time()\n",
        "      #     if 'hook_' in config['score']:\n",
        "\n",
        "      #         def counting_forward_hook(module_hook, inp, out):\n",
        "      #             try:\n",
        "      #                 if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "      #                     return\n",
        "      #                 if isinstance(inp, tuple):\n",
        "      #                     inp = inp[0]\n",
        "      #                 inp = inp.view(inp.size(0), -1)\n",
        "      #                 x = (inp > 0).float()\n",
        "      #                 K = x @ x.t()\n",
        "      #                 K2 = (1. - x) @ (1. - x.t())\n",
        "      #                 if hasattr(network, 'K'):\n",
        "      #                   network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "      #                 else: \n",
        "      #                   network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "      #             except Exception as exception:\n",
        "      #                 print(exception)\n",
        "      #                 pass\n",
        "\n",
        "\n",
        "      #         def counting_backward_hook(module_hook, inp, out):\n",
        "      #             module_hook.visited_backwards = True\n",
        "\n",
        "      #         j = []\n",
        "      #         for name, module in network.named_modules():\n",
        "      #             j.append(name)\n",
        "      #             if 'ReLU' in str(type(module)):\n",
        "      #                 module.register_forward_hook(counting_forward_hook)\n",
        "      #                 module.register_backward_hook(counting_backward_hook)\n",
        "      #     network = network.to(device)\n",
        "      s = get_synflow_score(uid)\n",
        "      # s = []\n",
        "      # for j in range(config['maxofn']):\n",
        "      #     data_iterator = iter(train_dt)\n",
        "      #     x, target = next(data_iterator)\n",
        "      #     x2 = torch.clone(x)\n",
        "      #     x2 = x2.to(device)\n",
        "      #     x, target = x.to(device), target.to(device)\n",
        "      #     jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "      #     if 'hook_' in config['score']:\n",
        "      #         network(x2.to(device))\n",
        "      #         value = hooklogdet(network.K, target)\n",
        "      #         s.append(value)\n",
        "      #     else:\n",
        "      #         value = hooklogdet(network.K, target)\n",
        "      #         s.append(value)\n",
        "      acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "      print(f'Score (uid {uid}): {s}, Accuracy: {acc}')\n",
        "      stop = time.time()\n",
        "      result['uid'].append(uid)\n",
        "      result['score'].append(s)\n",
        "      result['accuracy'].append(acc)\n",
        "      result['run_id'].append(run_id)\n",
        "      score = s\n",
        "      result['elapsed_time'].append(stop-start)\n",
        "      # except Exception as e:\n",
        "      #     print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': [], 'run_id':[]}\n",
        "    return df"
      ],
      "metadata": {
        "id": "UE1SaX2Z7XuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Architecture Configuration Class\n",
        "\"\"\"\n",
        "\n",
        "class Architecture:\n",
        "    def __init__(self, arch):\n",
        "        self.architecture = arch\n",
        "        self.graph = {'0': [], '1':[], '2': []}\n",
        "        for token in arch.split('+'):\n",
        "            tmp = list(filter(lambda x: x != '', token.split('|')))\n",
        "            for x in tmp:\n",
        "                op = x.split('~')\n",
        "                self.graph[str(op[1])].append(op[0])\n",
        "            \n",
        "    def distance(self, obj):\n",
        "        differences = 0\n",
        "        if isinstance(obj, Architecture):\n",
        "            for key in self.graph.keys():\n",
        "                differences += len([i for i, j in zip(self.graph[key], obj.graph[key]) if i != j])\n",
        "            # print('DIFF:' + str(differences))\n",
        "            return differences\n",
        "        else:\n",
        "            print('Invalid Object')\n",
        "            return -1\n",
        "            \n",
        "    def print_architecture(self):\n",
        "        print(self.architecture)"
      ],
      "metadata": {
        "id": "z2G3JAktoY7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Find all architectures with a distance n (max), the distance is calculated as Hamming Distance\n",
        "\"\"\"\n",
        "\n",
        "def find_arch_n_dist(survivors, max_dist, anchestors):\n",
        "  population = []\n",
        "  for uid in survivors['uid']:\n",
        "    net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "    net_config: dict = get_standard_config(net_config)\n",
        "    net_config = Architecture(net_config['arch_str'])\n",
        "    for id in range(0, max_uid):\n",
        "      candidate: pd.DataFrame = searchspace.loc[searchspace['uid'] == id]\n",
        "      candidate: dict = get_standard_config(candidate)\n",
        "      candidate = Architecture(candidate['arch_str'])\n",
        "      dist = net_config.distance(candidate)\n",
        "      # print('Distance: ' + str(dist))\n",
        "      # if id not in anchestors and dist <= max_dist:\n",
        "      if dist <= max_dist:\n",
        "        anchestors.append(id)\n",
        "        population.append(id)\n",
        "\n",
        "  population = random.sample(population, config['population_size']) if len(population) > config['population_size'] else []\n",
        "  population.append(*survivors['uid'])\n",
        "  return population\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "56IMvMlroY7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get N Random Samples\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = config['dataset']\n",
        "population = random.sample(range(max_uid), config['n_random'])\n",
        "trial = config['n_evolution']\n",
        "survivors = {}\n",
        "anchestors = population.copy()\n",
        "# Run Algorithm on Population\n",
        "start = time.time()\n",
        "\n",
        "print('*******************************')\n",
        "print('Running Regularized Evolution algorithm')\n",
        "print('Parameters:')\n",
        "print(f'Dataset: {dataset}')\n",
        "print(f'Num Round: {trial}')\n",
        "print(f'Scoring Algorithm: {proxy_type}')\n",
        "print('*******************************')\n",
        "for i in range(config['n_evolution']):\n",
        "  if config['proxy_type'] == 'ReLU':\n",
        "    trained_population = naswot_search(dataset, device, population)\n",
        "  else:\n",
        "    trained_population = synflow_search(dataset, device, population)\n",
        "  print('TRAINED', trained_population)\n",
        "  # Take N Survivor\n",
        "  trained_population.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "  survivors = trained_population.head(config['n_survivor'])\n",
        "  print('SURVIVORS', survivors)\n",
        "  # Create New Generation\n",
        "  if i < trial:\n",
        "    population = find_arch_n_dist(survivors=survivors, max_dist=config['n_arch_distance'], anchestors=anchestors)\n",
        "  else: \n",
        "    break\n",
        "  if len(population) == 0:\n",
        "    break\n",
        "\n",
        "  print(len(population), population)\n",
        "\n",
        "stop = time.time()\n",
        "\n",
        "total_time = stop - start\n",
        "print('*****************************************************************')\n",
        "print(f'Best performing net with RandomSearch')\n",
        "print(tabulate(survivors, headers='keys', tablefmt='psql', showindex=False))\n",
        "print(f'Total time for search over all searchspace: {total_time}')\n",
        "print('*****************************************************************')\n",
        "survivors.to_csv('NASWOT_REA.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L5VGzlhvoY7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}