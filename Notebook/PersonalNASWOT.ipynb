{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonalNASWOT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "18HTbJAkLy-Va3rkPUo2_HK-_rSmoCRw1",
      "authorship_tag": "ABX9TyMFz80UXgBp3Wq0feSVdu2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelePresti/NAS_MachineLearningDeepLearning/blob/main/PersonalNASWOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "hwKbWSOoYHoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchviz import make_dot\n",
        "\n",
        "\"\"\" TENSOR CREATION \"\"\"\n",
        "\n",
        "# data = [[1, 2], [3, 4]]\n",
        "# x_data = torch.tensor(data)\n",
        "\n",
        "# np_array = np.array(data)\n",
        "# x_np = torch.from_numpy(np_array)\n",
        "\n",
        "# x_ones = torch.ones_like(x_data)\n",
        "# # print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "# x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
        "# # print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "# shape = (2, 3,)\n",
        "# rand_tensor = torch.rand(shape)\n",
        "# ones_tensor = torch.ones(shape)\n",
        "# zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "# # print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "# # print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "# # print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
        "\n",
        "# tensor = torch.rand(3, 4)\n",
        "\n",
        "# # print(f\"Shape of tensor: {tensor.shape}\")\n",
        "# # print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "# # print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "# # We move our tensor to the GPU if available\n",
        "# if torch.cuda.is_available():\n",
        "#     tensor = tensor.to('cuda')\n",
        "#     # print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "# # Tensor Slicing\n",
        "# tensor = torch.ones(4, 4)\n",
        "# tensor[:, 1] = 0\n",
        "# # print(tensor)\n",
        "\n",
        "# # Join Tensor\n",
        "# t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "# # print(t1)\n",
        "\n",
        "# # This computes the element-wise product\n",
        "# # print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# # Alternative syntax:\n",
        "# # print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
        "\n",
        "# # print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "# # Alternative syntax:\n",
        "# # print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n",
        "\n",
        "# # In-place operations Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.\n",
        "# # print(tensor, \"\\n\")\n",
        "# tensor.add_(5)\n",
        "# # print(tensor)\n",
        "\n",
        "# A GENTLE INTRODUCTION TO TORCH.AUTOGRAD\n",
        "\n",
        "# For this example, we load a pretrained resnet18 model from torchvision.\n",
        "# We create a random data tensor to represent a single image with 3 channels, and height & width of 64,\n",
        "# and its corresponding label initialized to some random values. Label in pretrained models has shape (1,1000)\n",
        "# model = torchvision.models.resnet18(pretrained=True)\n",
        "# data = torch.rand(1, 3, 64, 64)\n",
        "# labels = torch.rand(1, 1000)\n",
        "\n",
        "# # Next, we run the input data through the model through each of its layers to make a prediction. This is the forward pass.\n",
        "# prediction = model(data)\n",
        "\n",
        "# # We use the model’s prediction and the corresponding label to calculate the error (loss).\n",
        "# # The next step is to backpropagate this error through the network.\n",
        "# # Backward propagation is kicked off when we call .backward() on the error tensor.\n",
        "# # Autograd then calculates and stores the gradients for each model parameter in the parameter’s .grad attribute.\n",
        "# loss = (prediction - labels).sum()\n",
        "# loss.backward()\n",
        "\n",
        "# # Next, we load an optimizer, in this case SGD with a learning rate of 0.01 and momentum of 0.9. We register all the parameters of the model in the optimizer.\n",
        "# optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "\n",
        "# # Finally, we call .step() to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in .grad.\n",
        "# optim.step()\n",
        "\n",
        "# make_dot(model(data)) # .render(\"rnn_torchviz\", format=\"png\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# net = Net()\n",
        "# print(net)\n",
        "\n",
        "# params = list(net.parameters())\n",
        "\n",
        "# input = torch.randn(1, 1, 32, 32)\n",
        "# out = net(input)\n",
        "# # make_dot(out)\n",
        "# # print(out)\n",
        "\n",
        "# net.zero_grad()\n",
        "# out.backward(torch.randn(1, 10))\n",
        "\n",
        "# output = net(input)\n",
        "# target = torch.randn(10)  # a dummy target, for example\n",
        "# target = target.view(1, -1)  # make it the same shape as output\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# loss = criterion(output, target)\n",
        "# print(loss)\n",
        "\n",
        "# print(loss.grad_fn)  # MSELoss\n",
        "# print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "# print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
        "\n",
        "# net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "# print('conv1.bias.grad before backward')\n",
        "# print(net.conv1.bias.grad)\n",
        "\n",
        "# loss.backward()\n",
        "\n",
        "# print('conv1.bias.grad after backward')\n",
        "# print(net.conv1.bias.grad)\n",
        "\n",
        "# import torch.optim as optim\n",
        "\n",
        "# # create your optimizer\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# # in your training loop:\n",
        "# optimizer.zero_grad()   # zero the gradient buffers\n",
        "# output = net(input)\n",
        "# loss = criterion(output, target)\n",
        "# loss.backward()\n",
        "# optimizer.step()    # Does the update\n",
        "\n"
      ],
      "metadata": {
        "id": "Oc0XPiR5X5Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timeit\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(12, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "for name, module in net.named_modules():\n",
        "    print(name, module)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "start = timeit.default_timer()\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "exec_time = stop - start;\n",
        "print('Finished Training')\n",
        "print(f'Execution Time: [{exec_time}]')\n",
        "\n",
        "\n",
        "# Save Result\n",
        "# PATH = './cifar_net.pth'\n",
        "# torch.save(net.state_dict(), PATH)\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "outputs = net(images)\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
        "\n",
        "print(net.named_modules())"
      ],
      "metadata": {
        "id": "r5pAFEAAgafj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/nas-wot-lib/NAS_MachineLearningDeepLearning/main.py ."
      ],
      "metadata": {
        "id": "c3eee9IWT1J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nas-bench-201"
      ],
      "metadata": {
        "id": "YTbXKyZWk-kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAS WOT Algorithm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "from searchspace.searchspace import NasBench201\n",
        "from datasets import datasets as data\n",
        "from score.score import ScoreAgent as Score\n",
        "\n",
        "# PARSING ARGUMENT\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='NAS Without Training')\n",
        "# parser.add_argument('--data_loc', default='../cifardata/', type=str, help='dataset folder')\n",
        "# parser.add_argument('--api_loc', default='../NAS-Bench-201-v1_0-e61699.pth',\n",
        "#                     type=str, help='path to API')\n",
        "# parser.add_argument('--save_loc', default='results', type=str, help='folder to save results')\n",
        "# parser.add_argument('--save_string', default='naswot', type=str, help='prefix of results file')\n",
        "# parser.add_argument('--score', default='hook_logdet', type=str, help='the score to evaluate')\n",
        "# parser.add_argument('--nasspace', default='nasbench201', type=str, help='the nas search space to use')\n",
        "# parser.add_argument('--batch_size', default=128, type=int)\n",
        "# parser.add_argument('--repeat', default=1, type=int, help='how often to repeat a single image with a batch')\n",
        "# parser.add_argument('--augtype', default='none', type=str, help='which perturbations to use')\n",
        "# parser.add_argument('--sigma', default=0.05, type=float, help='noise level if augtype is \"gaussnoise\"')\n",
        "# parser.add_argument('--GPU', default='0', type=str)\n",
        "# parser.add_argument('--seed', default=1, type=int)\n",
        "# parser.add_argument('--init', default='', type=str)\n",
        "# parser.add_argument('--trainval', action='store_true')\n",
        "# parser.add_argument('--dropout', action='store_true')\n",
        "# parser.add_argument('--dataset', default='cifar10', type=str)\n",
        "# parser.add_argument('--maxofn', default=1, type=int, help='score is the max of this many evaluations of the network')\n",
        "# parser.add_argument('--n_samples', default=100, type=int)\n",
        "# parser.add_argument('--n_runs', default=500, type=int)\n",
        "# parser.add_argument('--stem_out_channels', default=16, type=int,\n",
        "#                     help='output channels of stem convolution (nasbench101)')\n",
        "# parser.add_argument('--num_stacks', default=3, type=int, help='#stacks of modules (nasbench101)')\n",
        "# parser.add_argument('--num_modules_per_stack', default=3, type=int, help='#modules per stack (nasbench101)')\n",
        "# parser.add_argument('--num_labels', default=1, type=int, help='#classes (nasbench101)')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "#######################\n",
        "\n",
        "# LOADING SEARCH SPACE ########\n",
        "path = '/content/drive/MyDrive/NAS-Bench-201-v1_1-096897.pth'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = 'cifar10'\n",
        "\n",
        "searchspace = NasBench201(path)\n",
        "\n",
        "################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tS4pe75zT-Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING DATASET ##############\n",
        "\n",
        "\"\"\" Modules for data loading can be found in datasets package\"\"\"\n",
        "\n",
        "train_dt = data.load_dataset(dataset)\n",
        "print(train_dt)\n",
        "\n",
        "################################"
      ],
      "metadata": {
        "id": "WwxQaIGAkv2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCORE INITIALIZING ##############\n",
        "\n",
        "score = Score(searchspace)\n",
        "print(score)\n",
        "\n",
        "################################"
      ],
      "metadata": {
        "id": "mCkU3JxRky_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCORING ARCHITECTURE #########\n",
        "\n",
        "for i, (uid, network) in enumerate(searchspace):\n",
        "    try:\n",
        "\n",
        "        # Add Hook to modules in the current network\n",
        "        if 'hook_' in args.score:\n",
        "\n",
        "            def counting_forward_hook(module_hook, inp, out):\n",
        "                try:\n",
        "                    if not module_hook.visited_backwards:\n",
        "                        return\n",
        "                    if isinstance(inp, tuple):\n",
        "                        inp = inp[0]\n",
        "                    inp = inp.view(inp.size(0), -1)\n",
        "                    x = (inp > 0).float()\n",
        "                    K = x @ x.t()\n",
        "                    K2 = (1. - x) @ (1. - x.t())\n",
        "                    network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                except Exception as exception:\n",
        "                    print(exception)\n",
        "                    pass\n",
        "\n",
        "            def counting_backward_hook(module_hook, inp, out):\n",
        "                module_hook.visited_backwards = True\n",
        "\n",
        "            for name, module in network.named_modules():\n",
        "                if 'ReLU' in str(type(module)):\n",
        "                    module.register_forward_hook(counting_forward_hook)\n",
        "                    module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "        # Starting score algorithm\n",
        "        network = network.to(device)\n",
        "        random.seed(args.seed)\n",
        "        np.random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        s = []\n",
        "        for j in range(args.maxofn):\n",
        "            data_iterator = iter(train_dt)\n",
        "            x, target = next(data_iterator)\n",
        "            x2 = torch.clone(x)\n",
        "            x2 = x2.to(device)\n",
        "            x, target = x.to(device), target.to(device)\n",
        "            print(x, target)\n",
        "            jacobs, labels, y, out = score.get_jacobian(network, x, target, device, args)\n",
        "            if 'hook_' in args.score:\n",
        "                network(x2.to(device))\n",
        "                s.append(score.get_score_func(args.score)(network.K, target))\n",
        "            else:\n",
        "                pass\n",
        "                s.append(score.get_score_func(args.score)(jacobs, labels))\n",
        "\n",
        "        score.register_score(s, i)\n",
        "        \"\"\" Check other score in original file and add \"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        score.register_score([np.NaN], i)\n",
        "\n",
        "################################\n",
        "\n",
        "# SAVING RESULT ################\n",
        "\n",
        "\"\"\" TO IMPLEMENT \"\"\"\n",
        "\n",
        "################################\n"
      ],
      "metadata": {
        "id": "sitHz48lk7DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = [1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 1, 11, 1, 1,1 ,1, 1, 1]"
      ],
      "metadata": {
        "id": "jrS13qdulS82"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
