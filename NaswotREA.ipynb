{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelePresti/NAS_MachineLearningDeepLearning/blob/main/NaswotREA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Nq2-kqUpgm"
      },
      "source": [
        "#Define Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ofIogUx_T3lD"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "max_uid = 15625\n",
        "#@title ##Configuration Info \n",
        "#configuration by param\n",
        "dataset = \"ImageNet16\" #@param {type:\"string\"} [\"cifar10\", \"cifar100\", \"ImageNet16\"]\n",
        "run_id =  1# @param {type:\"integer\"}\n",
        "trial =  30#@param {type:\"integer\"}\n",
        "n_random =  10#@param {type:\"integer\"}\n",
        "point = '2a' # @param ['2a', '2b']\n",
        "imagenet_path = 'Use only if dataset=Imagenet16' #@param{type:\"string\"}\n",
        "use_default_path = True #@param{type:\"boolean\"}\n",
        "n_evolution = 10#@param{type: \"integer\"}\n",
        "n_arch_distance = 6#@param{type: \"integer\"}\n",
        "n_survivor = 5#@param{type:\"integer\"}\n",
        "\n",
        "\n",
        "config['score'] = 'hook_logdet'\n",
        "config['nasspace'] = 'nasbench201'\n",
        "config['augtype'] = 'none'\n",
        "config['dataset'] = dataset\n",
        "config['maxofn'] = 3\n",
        "config['batch_size'] = 128\n",
        "config['seed'] = 1\n",
        "config['run_id'] = run_id\n",
        "config['dataset_id'] = 'CIFAR10'\n",
        "config['start_uid'] = 0 \n",
        "config['stop_uid'] =  15000 \n",
        "config['trial'] = trial\n",
        "config['n_random'] = n_random\n",
        "config['point'] = point\n",
        "config['imagenet_path'] = '/content/drive/MyDrive/ImageNet16' if use_default_path else imagenet_path\n",
        "config['n_evolution'] = n_evolution\n",
        "config['n_arch_distance'] = n_arch_distance\n",
        "config['n_survivor'] = n_survivor\n",
        "\n",
        "#max 15625 stop_uid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h1xetWeZeEDF",
        "outputId": "f1c22970-7781-422f-c647-fd60dfc21c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hov87k0BUwXP"
      },
      "source": [
        "#Import NAS Bench API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVF4reTyT-_g"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/MichelePresti/NAS_MachineLearningDeepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOSiDI1bUBNA"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/NAS_MachineLearningDeepLearning/neural_model ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2IG2PhLUCaw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_arch_config_by_dataset(dataset) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function return the architectures config by dataset in a pandas dataframe.\n",
        "    PARAMETERS:\n",
        "       dataset= string among [cifar10, cifar100, imaginet]\n",
        "    \"\"\"\n",
        "    if(dataset == 'cifar10'):\n",
        "        df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__CIFAR10_config.csv', header=0)\n",
        "        return df\n",
        "    if(dataset == 'cifar100'):\n",
        "      df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__CIFAR100_config.csv', header=0)\n",
        "      return df\n",
        "    if(dataset == 'ImageNet16'):\n",
        "      df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__ImageNet16_config.csv', header=0)\n",
        "      return df\n",
        "    else: \n",
        "      print('Dataset name not valid')\n",
        "      return None\n",
        "\n",
        "def get_standard_config(csv_config: pd.DataFrame) -> dict:\n",
        "    res = {}\n",
        "    res['name'] = csv_config.iloc[0]['name']\n",
        "    res['C'] = csv_config.iloc[0]['C']\n",
        "    res['N'] = csv_config.iloc[0]['N']\n",
        "    res['arch_str'] = csv_config.iloc[0]['arch_str']\n",
        "    res['num_classes'] = 1\n",
        "    return res\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1oGWDNmgUCYP",
        "outputId": "4597ce4f-32d3-4211-d132-3e79d1a06863"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0    uid        name   C  N  \\\n",
              "0               0      0  infer.tiny  16  5   \n",
              "1               1      1  infer.tiny  16  5   \n",
              "2               2      2  infer.tiny  16  5   \n",
              "3               3      3  infer.tiny  16  5   \n",
              "4               4      4  infer.tiny  16  5   \n",
              "...           ...    ...         ...  .. ..   \n",
              "15620       15620  15620  infer.tiny  16  5   \n",
              "15621       15621  15621  infer.tiny  16  5   \n",
              "15622       15622  15622  infer.tiny  16  5   \n",
              "15623       15623  15623  infer.tiny  16  5   \n",
              "15624       15624  15624  infer.tiny  16  5   \n",
              "\n",
              "                                                arch_str  num_classes  \n",
              "0      |avg_pool_3x3~0|+|nor_conv_1x1~0|skip_connect~...          120  \n",
              "1      |nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~...          120  \n",
              "2      |avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~...          120  \n",
              "3      |avg_pool_3x3~0|+|skip_connect~0|none~1|+|none...          120  \n",
              "4      |skip_connect~0|+|skip_connect~0|nor_conv_1x1~...          120  \n",
              "...                                                  ...          ...  \n",
              "15620  |none~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|skip...          120  \n",
              "15621  |avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~...          120  \n",
              "15622  |skip_connect~0|+|nor_conv_3x3~0|nor_conv_3x3~...          120  \n",
              "15623  |none~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|avg_...          120  \n",
              "15624  |nor_conv_1x1~0|+|none~0|nor_conv_1x1~1|+|none...          120  \n",
              "\n",
              "[15625 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f522da8b-6531-4921-9bce-12afa8bdf077\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>uid</th>\n",
              "      <th>name</th>\n",
              "      <th>C</th>\n",
              "      <th>N</th>\n",
              "      <th>arch_str</th>\n",
              "      <th>num_classes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|avg_pool_3x3~0|+|nor_conv_1x1~0|skip_connect~...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|avg_pool_3x3~0|+|skip_connect~0|none~1|+|none...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|skip_connect~0|+|skip_connect~0|nor_conv_1x1~...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15620</th>\n",
              "      <td>15620</td>\n",
              "      <td>15620</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|none~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|skip...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15621</th>\n",
              "      <td>15621</td>\n",
              "      <td>15621</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|avg_pool_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15622</th>\n",
              "      <td>15622</td>\n",
              "      <td>15622</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|skip_connect~0|+|nor_conv_3x3~0|nor_conv_3x3~...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15623</th>\n",
              "      <td>15623</td>\n",
              "      <td>15623</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|none~0|+|avg_pool_3x3~0|avg_pool_3x3~1|+|avg_...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15624</th>\n",
              "      <td>15624</td>\n",
              "      <td>15624</td>\n",
              "      <td>infer.tiny</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>|nor_conv_1x1~0|+|none~0|nor_conv_1x1~1|+|none...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15625 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f522da8b-6531-4921-9bce-12afa8bdf077')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f522da8b-6531-4921-9bce-12afa8bdf077 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f522da8b-6531-4921-9bce-12afa8bdf077');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "searchspace = get_arch_config_by_dataset(config['dataset'])\n",
        "searchspace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW5ewvBLU1OE"
      },
      "source": [
        "#Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s65hVshwUCVo"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019 #\n",
        "##################################################\n",
        "import os, sys, hashlib, torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "\n",
        "def calculate_md5(fpath, chunk_size=1024 * 1024):\n",
        "    md5 = hashlib.md5()\n",
        "    with open(fpath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "\n",
        "def check_md5(fpath, md5, **kwargs):\n",
        "    return md5 == calculate_md5(fpath, **kwargs)\n",
        "\n",
        "\n",
        "def check_integrity(fpath, md5=None):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    if md5 is None:\n",
        "        return True\n",
        "    else:\n",
        "        return check_md5(fpath, md5)\n",
        "\n",
        "\n",
        "class ImageNet16(data.Dataset):\n",
        "    # http://image-net.org/download-images\n",
        "    # A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets\n",
        "    # https://arxiv.org/pdf/1707.08819.pdf\n",
        "\n",
        "    train_list = [\n",
        "        [\"train_data_batch_1\", \"27846dcaa50de8e21a7d1a35f30f0e91\"],\n",
        "        [\"train_data_batch_2\", \"c7254a054e0e795c69120a5727050e3f\"],\n",
        "        [\"train_data_batch_3\", \"4333d3df2e5ffb114b05d2ffc19b1e87\"],\n",
        "        [\"train_data_batch_4\", \"1620cdf193304f4a92677b695d70d10f\"],\n",
        "        [\"train_data_batch_5\", \"348b3c2fdbb3940c4e9e834affd3b18d\"],\n",
        "        [\"train_data_batch_6\", \"6e765307c242a1b3d7d5ef9139b48945\"],\n",
        "        [\"train_data_batch_7\", \"564926d8cbf8fc4818ba23d2faac7564\"],\n",
        "        [\"train_data_batch_8\", \"f4755871f718ccb653440b9dd0ebac66\"],\n",
        "        [\"train_data_batch_9\", \"bb6dd660c38c58552125b1a92f86b5d4\"],\n",
        "        [\"train_data_batch_10\", \"8f03f34ac4b42271a294f91bf480f29b\"],\n",
        "    ]\n",
        "    valid_list = [\n",
        "        [\"val_data\", \"3410e3017fdaefba8d5073aaa65e4bd6\"],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, train, transform, use_num_of_class_only=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.train = train  # training set or valid set\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted.\")\n",
        "\n",
        "        if self.train:\n",
        "            downloaded_list = self.train_list\n",
        "        else:\n",
        "            downloaded_list = self.valid_list\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        for i, (file_name, checksum) in enumerate(downloaded_list):\n",
        "            file_path = os.path.join(self.root, file_name)\n",
        "            # print ('Load {:}/{:02d}-th : {:}'.format(i, len(downloaded_list), file_path))\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(f)\n",
        "                else:\n",
        "                    entry = pickle.load(f, encoding=\"latin1\")\n",
        "                self.data.append(entry[\"data\"])\n",
        "                self.targets.extend(entry[\"labels\"])\n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 16, 16)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        if use_num_of_class_only is not None:\n",
        "            assert (\n",
        "                isinstance(use_num_of_class_only, int)\n",
        "                and use_num_of_class_only > 0\n",
        "                and use_num_of_class_only < 1000\n",
        "            ), \"invalid use_num_of_class_only : {:}\".format(use_num_of_class_only)\n",
        "            new_data, new_targets = [], []\n",
        "            for I, L in zip(self.data, self.targets):\n",
        "                if 1 <= L <= use_num_of_class_only:\n",
        "                    new_data.append(I)\n",
        "                    new_targets.append(L)\n",
        "            self.data = new_data\n",
        "            self.targets = new_targets\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{name}({num} images, {classes} classes)\".format(\n",
        "            name=self.__class__.__name__,\n",
        "            num=len(self.data),\n",
        "            classes=len(set(self.targets)),\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index] - 1\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in self.train_list + self.valid_list:\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpvhhftTUCTS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "def get_dataset(dataset) -> DataLoader:\n",
        "    \"\"\"\n",
        "    This function return the dataset given its name in torch DataLoader format.\n",
        "    PARAMETERS:\n",
        "       dataset= string among [cifar10, cifar100, imaginet]\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset == 'cifar10':\n",
        "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    elif dataset == 'cifar100':\n",
        "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
        "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    elif dataset.startswith('ImageNet16'):\n",
        "        mean = [x / 255 for x in [122.68, 116.66, 104.01]]\n",
        "        std = [x / 255 for x in [63.22, 61.26, 65.09]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(16, padding=2), transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = ImageNet16(config['imagenet_path'], True, transform, 120)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    else:\n",
        "        raise TypeError(\"Unknow dataset : {:}\".format(dataset))\n",
        "\n",
        "    return train_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZRyJjrpUCQn"
      },
      "outputs": [],
      "source": [
        "train_dt = get_dataset(config['dataset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7EkocqlU5P7"
      },
      "source": [
        "#Define Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqoS_7YTUCOC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_batch_jacobian(net, x, target, device, args=None):\n",
        "    net.zero_grad()\n",
        "    x.requires_grad_(True)\n",
        "    y, out = net(x)\n",
        "    y.backward(torch.ones_like(y))\n",
        "    jacob = x.grad.detach()\n",
        "    return jacob, target.detach(), y.detach(), out.detach()\n",
        "\n",
        "\n",
        "def hooklogdet(K, labels=None):\n",
        "    s, ld = np.linalg.slogdet(K)\n",
        "    return ld\n",
        "\n",
        "\n",
        "def score_network(network, x, x2, target, device):\n",
        "    jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "    network(x2.to(device))\n",
        "    value = hooklogdet(network.K, target)\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLWrtL3dU7uf"
      },
      "source": [
        "#Search Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.telegram import tqdm, trange\n",
        "\n",
        "def naswot_search_n2(run_id, dataset_id, device, n):\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    n = config['n_random']\n",
        "    trial = config['trial']\n",
        "    point = config['point']\n",
        "    l = random.sample(range(max_uid), n)\n",
        "    # for uid in tqdm(l,\\\n",
        "    #    token='5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw',\\\n",
        "    #    chat_id='142397010',\\\n",
        "    #    desc=f'result_run{run_id}_dataset{dataset_id}_point_{point}'):\n",
        "    for uid in l:\n",
        "      #i = uid-start_uid\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "          #print('NUM MODULES IN ARCHITECTURES', len(j))\n",
        "          # Starting score algorithm\n",
        "          network = network.to(device)\n",
        "          ## start time\n",
        "          #start = time.time()\n",
        "          random.seed(config['seed'])\n",
        "          np.random.seed(config['seed'])\n",
        "          torch.manual_seed(config['seed'])\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          #print(f'Score (uid {uid}): {np.mean(s)}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "          #print(f'Elapsed time (uid {uid}): {stop-start}')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    df.to_csv(f'./result_run{run_id}_{trial}_{dataset_id}.csv')\n",
        "    \n",
        "    return "
      ],
      "metadata": {
        "id": "pFNEIq332qv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2a or 2b"
      ],
      "metadata": {
        "id": "gYVmXBTPmyCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/Cifar10Result.csv')\n",
        "try: \n",
        "  df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "except:\n",
        "  print(\"Already dropped\")\n",
        "\n",
        "acc_df = df\n"
      ],
      "metadata": {
        "id": "9EfGHBULm08R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.telegram import tqdm, trange\n",
        "\n",
        "\n",
        "def naswot_search_n(run_id, dataset, device, n, trial):\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    n = config['n_random']\n",
        "    #trial = config['trial']\n",
        "    #random.seed(i)\n",
        "    l = random.sample(range(max_uid), n)\n",
        "    best_score, best_acc, best_net = 0, 0, 0\n",
        "    best = {}\n",
        "    best['score'] = 0\n",
        "    best['acc'] = 0\n",
        "    best['uid'] = 0\n",
        "    print(l)\n",
        "    # for uid in tqdm(l,\\\n",
        "    #    token='5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw',\\\n",
        "    #    chat_id='142397010',\\\n",
        "    #    desc=f'result_run{run_id}_{point}_dataset{dataset_id}_'):\n",
        "    for uid in l:\n",
        "      #i = uid-start_uid\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "          #print('NUM MODULES IN ARCHITECTURES', len(j))\n",
        "          # Starting score algorithm\n",
        "          \n",
        "          network = network.to(device)\n",
        "          ## start time\n",
        "          #start = time.time()\n",
        "          #random.seed(config['seed'])\n",
        "          #np.random.seed(config['seed'])\n",
        "          #torch.manual_seed(config['seed'])\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "          print(f'Score (uid {uid}): {np.mean(s)}, Accuracy: {acc}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          score = np.mean(s)\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "\n",
        "          if config['point'] == '2a':\n",
        "          #point 2a\n",
        "            if score > best_score:\n",
        "              best_score = score\n",
        "              best_net = uid\n",
        "              best_acc = acc\n",
        "          else:\n",
        "            #point 2b\n",
        "            \n",
        "            if acc > best_acc:\n",
        "              best_acc = acc\n",
        "              best_score = score\n",
        "              best_net = uid\n",
        "\n",
        "          #print(f'Elapsed time (uid {uid}): {stop-start}')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    df.to_csv(f'./result_run{run_id}_trial{trial}_{point}_{dataset}.csv')\n",
        "    print(f'Best Network: {best_net}; Score: {best_score}; Accuracy: {best_acc}')\n",
        "    best = {}\n",
        "    best['acc'] = best_acc\n",
        "    best['score'] = best_score\n",
        "    best['uid'] = best_net\n",
        "    series_net = pd.Series(best)\n",
        "    series_net.to_csv(f'best_network_{run_id}_trial{trial}_{dataset}.csv')\n",
        "    return "
      ],
      "metadata": {
        "id": "4w8L81_mnF_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#All Search"
      ],
      "metadata": {
        "id": "fyQKB2AfZmgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coGyuvqXUCLf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.telegram import tqdm, trange\n",
        "\n",
        "def search_all(run_id, dataset_id, device, start_uid, stop_uid):\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    #l = random.sample(range(max_uid), 100)\n",
        "    #for uid in tqdm(range(start_uid, stop_uid)):\n",
        "    for uid in tqdm(range(start_uid, stop_uid),\\\n",
        "       token='5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw',\\\n",
        "       chat_id='142397010',\\\n",
        "       desc=f'result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_'):\n",
        "      i = uid-start_uid\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "          #print('NUM MODULES IN ARCHITECTURES', len(j))\n",
        "          # Starting score algorithm\n",
        "          network = network.to(device)\n",
        "          ## start time\n",
        "          #start = time.time()\n",
        "          random.seed(config['seed'])\n",
        "          np.random.seed(config['seed'])\n",
        "          torch.manual_seed(config['seed'])\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          #print(f'Score (uid {uid}): {np.mean(s)}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "          #print(f'Elapsed time (uid {uid}): {stop-start}')\n",
        "          if i + 1 % 1000 == 0:\n",
        "            df = pd.DataFrame.from_dict(result)\n",
        "            result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "            df.to_csv(f'./NASWOT_result_run{run_id}_dataset{dataset_id}_{i}_.csv')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    df.to_csv(f'./result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_LastRecords_.csv')\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiuTUsbqU_LA"
      },
      "source": [
        "#Run Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def saving_results(trial):\n",
        "  path = f'./result_run{run_id}_trial{trial}_{point}_{dataset}.csv'\n",
        "  path_net = f'best_network_{run_id}_trial{trial}_{dataset}.csv'\n",
        "  \n",
        "  if point == '2a':\n",
        "    !cp -r $path_net /content/drive/MyDrive/project/2a/best\n",
        "    !cp -r $path /content/drive/MyDrive/project/2a/result\n",
        "    !rm $path\n",
        "    !rm $path_net\n",
        "  elif point == '2b':\n",
        "    !cp -r $path_net /content/drive/MyDrive/project/2b/best\n",
        "    !cp -r $path /content/drive/MyDrive/project/2b/result\n",
        "    !rm $path\n",
        "    !rm $path_net\n",
        "  # send_requests(trial)\n",
        "  return\n",
        "\n",
        "\n",
        "def send_requests(trial):\n",
        "  TOKEN = '5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw'\n",
        "  CHAT_ID = '142397010'\n",
        "  SEND_URL = f'https://api.telegram.org/bot{TOKEN}/sendMessage'\n",
        "  your_message = f\"RUN COMPLETED!\\nInformation:\\n\\t\\t\\t\\t\\t\\t\\t\\tRun_Id:\\t{run_id}\\n\\t\\t\\t\\t\\t\\t\\t\\tTrial:{trial}\\n\\t\\t\\t\\t\\t\\t\\t\\tDataset_Id:\\t{dataset_id}\"\n",
        "  requests.post(SEND_URL, json={'chat_id': CHAT_ID, 'text': your_message}) \n",
        "  return"
      ],
      "metadata": {
        "id": "8CAsOTpzXDGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIh6uy9bUCI4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import random\n",
        "import os\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ['WANDB_CONSOLE'] = 'off'\n",
        "start = time.time()\n",
        "\n",
        "run_id = config['run_id']\n",
        "dataset = config['dataset']\n",
        "start_uid = config['start_uid']\n",
        "stop_uid = config['stop_uid']\n",
        "n = config['n_random']    # N size of random sample\n",
        "trial = config['trial']\n",
        "\n",
        "rand_range = random.sample(range(1000), trial)\n",
        "print(rand_range)\n",
        "trial = 0\n",
        "#search_all(run_id, dataset_id, device, start_uid, stop_uid)\n",
        "for i in rand_range:\n",
        "  trial += 1\n",
        "  config['seed'] = i\n",
        "  naswot_search_n(run_id, dataset, device, n, trial)\n",
        "  saving_results(trial)\n",
        "\n",
        "\n",
        "stop = time.time()\n",
        "\n",
        "total_time = stop - start\n",
        "\n",
        "print(f'Total time for search over all searchspace: {total_time}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdWImgk0VCKr"
      },
      "source": [
        "#Save Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJd_rt9sUCGb"
      },
      "outputs": [],
      "source": [
        "#path = f'./result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_LastRecords_.csv'\n",
        "path = f'./result_run{run_id}_{trial}_{dataset_id}.csv'\n",
        "path_net = f'best_network_{run_id}_trial{trial}_{dataset_id}.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cX8kz0hUCDm"
      },
      "outputs": [],
      "source": [
        "!cp -r $path /content/drive/MyDrive/project\n",
        "!cp -r $path_net /content/drive/MyDrive/project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-3jXt0ghbqj"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# import requests\n",
        "\n",
        "# TOKEN = '5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw'\n",
        "# CHAT_ID = '142397010'\n",
        "# SEND_URL = f'https://api.telegram.org/bot{TOKEN}/sendMessage'\n",
        "# your_message = f\"RUN COMPLETED!\\nInformation:\\n\\t\\t\\t\\t\\t\\t\\t\\tRun_Id:\\t{run_id}\\n\\t\\t\\t\\t\\t\\t\\t\\tTrial:{trial}\\n\\t\\t\\t\\t\\t\\t\\t\\tDataset_Id:\\t{dataset_id}\"\n",
        "# requests.post(SEND_URL, json={'chat_id': CHAT_ID, 'text': your_message}) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aging Evolution Algorithm 🥇\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "\n",
        "1.   Get N Random Architectures Called Population\n",
        "2.   Run Scoring Algorithm On Population\n",
        "3.   Take N Survivor, Choose As The Best Score\n",
        "4.   Create New Generation With Architecture At N Distance From The Survivor\n",
        "5.   Repeat From 2 For N Evolution Era\n",
        "\n"
      ],
      "metadata": {
        "id": "45pD-uhihnVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "\n",
        "\"\"\"\n",
        "Find all architectures with a distance n (max), the distance is calculated as Hamming Distance\n",
        "\"\"\"\n",
        "def find_arch_n_dist(arch, n):\n",
        "  print(arch)\n",
        "  return []\n"
      ],
      "metadata": {
        "id": "ZYKfushphyQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from neural_model.neural_model import get_cell_net\n",
        "\n",
        "\"\"\"\n",
        "NAS WOT Algorithm\n",
        "\"\"\"\n",
        "\n",
        "def naswot_search(dataset, device, population) -> pd.DataFrame:\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    for uid in population:\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "          network = network.to(device)\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "          print(f'Score (uid {uid}): {np.mean(s)}, Accuracy: {acc}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          score = np.mean(s)\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    return df"
      ],
      "metadata": {
        "id": "G18K3jznCcdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get N Random Samples\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = config['dataset']\n",
        "population = random.sample(range(max_uid), config['n_random'])\n",
        "\n",
        "# Run Algorithm on Population\n",
        "\n",
        "for i in range(config['n_evolution']):\n",
        "  trained_population = naswot_search(dataset, device, population)\n",
        "\n",
        "  # Take N Survivor\n",
        "  trained_population.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "  survivors = trained_population.head(config['n_survivor'])\n",
        "\n",
        "# Create New Generation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YxMZA3Damyvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNiyQzHUGObL",
        "outputId": "bba32034-8bd0-4548-ba1f-feadf939ab88"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NaswotREA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hov87k0BUwXP",
        "CW5ewvBLU1OE",
        "C7EkocqlU5P7",
        "kLWrtL3dU7uf",
        "fyQKB2AfZmgc",
        "SdWImgk0VCKr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}